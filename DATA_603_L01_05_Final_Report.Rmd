---
title: "DATA 603 L01 05 Final Report"
output:
  pdf_document: default
  html_notebook: default
  word_document: default
---

\begin{titlepage}
  \centering
  \vspace*{1in}
  {\LARGE \bfseries DATA 603 L01 05 Final Report} \\[1.5cm]
  {\huge \bfseries HealthMatrix - Predicting Weight Change Using Diet Analysis} \\[2cm]
  
  \begin{flushright}
    {\Large Steen Rasmussen - 30097313} \\
    {\Large Aaron Gelfand - 10032214} \\
    {\Large Jackson Meier - 30095291} \\
    {\Large Harpreet Saini - 30271048} \\
    {\Large Venkateshwaran Balu Soundararajan - 30239509}
  \end{flushright}
  
  \vfill
  {\large \today}
\end{titlepage}

## 1 Introduction

For our project, we will be analyzing various factors that influence weight change using a multilinear regression model. Our dataset contains information relating to diet, physical activity and various lifestyle habits that are known to influence weight fluctuations. Through our analysis, we hope to identify what factors should be limited to prevent weight gain, and what factors should be encouraged to promote weight loss. Through this analysis, we hope to provide insight and various suggestions as to how to properly manage weight from both individual and public health perspectives. Obesity has become an extremely important topic in recent years, causing more health problems than we have ever seen before[^1]. Our main objective will be to predict weight changes based on these lifestyle and dietary variables. Through the use of our multilinear regression model, we will identify the significant variables that influence weight change, eliminate insignificant variables, resulting in a consistent and reliable framework to determine weight change. Other studies in this area identify lifestyle habits such as caloric intake, physical activity and quantity of sleep as important factors[^2].

We expect our analysis to suggest that a caloric deficit contributes to weight gain, and vice versa, as well as higher physical activity level, quality sleep, and lower stress levels to contribute positively to weight loss. Duration (in weeks) will also be an important factor, but could prove to be insignificant depending on how well each participant scores in the other areas.

## 2 Methodology

### 2.1 Data

The dataset[^3] is publicly available as open-source data on Kaggle, it was provided by an external source and was not collected or created by any members of our group. The team is using this dataset only for analysis and research purposes, with no changes in its original data collection processes. The dataset is free to access publicly without any licensing and permission required, but the credentials are mentioned. 

This dataset comprises information from 100 participants, focusing on demographics, dietary habits, physical activity levels, and lifestyle factors to predict weight change over time. Key features include age, gender, current weight, daily caloric intake, macronutrient breakdown, sleep quality, and stress levels. Based on this we aimed to analyze how these variables interact and influence weight fluctuations, providing a valuable resource for researchers and practitioners in nutrition and health. 

Based on the analysis we have categorized the qualitative and quantitative variables as below based on the data set

**Table 1. Variable Information**

| S. No| Variable Name                 |   Variable Type  |   Comments        | Possible Values                             |
|------|-------------------------------|------------------|-------------------|---------------------------------------------|
|   1  | Participant ID                |   Quantitative   |   Auto Increment  |                                             |
|   2  | Weight Change (lbs)           |   Quantitative   |   Response        |                                             |
|   3  | Age                           |   Quantitative   |   Predictor       |                                             |
|   4  | Current Weight (lbs)          |   Quantitative   |   Predictor       |                                             |
|   5  | BMR (Calories)                |   Quantitative   |   Predictor       |                                             |
|   6  | Daily Calories Consumed       |   Quantitative   |   Predictor       |                                             |
|   7  | Daily Caloric Surplus/Deficit |   Quantitative   |   Predictor       |                                             |
|   8  | Duration (weeks)              |   Quantitative   |   Predictor       |                                             |
|   9  | Final Weight (lbs)            |   Quantitative   |   Predictor       |                                             |
|   10 | Gender                        |   Categorical    |   Predictor       | 2 Level: M, F                              |
|   11 | Physical Activity Level       |   Categorical    |   Predictor       | 4 Level: Sedentary, Very Active, Lightly Active, Moderately Active |
|   12 | Sleep Quality                 |   Categorical    |   Predictor       | 4 Level: Excellent, Good, Fair, Poor       |
|   13 | Stress Level                  |   Categorical    |   Predictor       | 9 Level: Range (1,9)                      |

### 2.2 Approach

The primary objective of this project is to identify the key factors that significantly influence weight changes in the human body. To achieve this, we will build multiple models based on the following guiding questions: 

<u>**Guiding Question 1:**</u> "Analyze how do gender, Physical Activity Level, Sleep Quality, and Stress Level, Age, Current Weight, BMR, Daily Calories Consumed, Daily Caloric Surplus/Deficit and Duration of the 100 participants collectively influence weight change, and are there significant interactions between these factors that modify their effects on weight change?‚Äù 

<u>**Guiding Question 2:**</u> "Analyze how do age, basal metabolic rate, daily caloric intake, and caloric surplus or deficit affect weight change over the program's duration? Are there any combinations of these factors that are more strongly connected with weight change?" 

<u>**Guiding Question 3:**</u> " Analyze how gender, physical activity level, sleep quality, and stress level affect weight change in adults, both individually and in combination?" For example, does increased physical activity lower weight more effectively in low-stress situations, or does excellent sleep quality play a larger impact at specific stress and activity levels?"

Hence based on the interpretation of these model we will determine the highest factors that play a vital role to reflect the weight changes in the human body and derive the best fit model for weight prediction 

### 2.3 Workflow

<u>**Workflow Task List:**</u>

**Step 1: Data Loading and Wrangling **
The first step in any data analysis project is to load the dataset and perform necessary data wrangling. This involves cleaning, preprocessing, and transforming the data to ensure accuracy and reliability. Using R, we can load the dataset into a data frame and apply various transformations to prepare the data for analysis. 

**Step 2: Variable Identification and Removal**
Before performing any regression tests, it's crucial to identify and remove variables that do not support the modelling process. This includes auto-increment variables, index variables, and any metadata information. Removing these variables helps in focusing on the relevant predictors and improves the model's performance. 

**Step 3: Building the First Order Model** 
With the cleaned dataset, we build a first-order model using linear regression. This model helps in understanding the relationship between the dependent variable and the independent variables. We then perform individual T-tests to evaluate the significance of each predictor. 

**Step 4: Residual Analysis and Multicollinearity Check** 
Residual analysis is essential to check the assumptions of regression. We plot the residuals to ensure they are randomly distributed. Additionally, we use the Variance Inflation Factor (VIF) method to test for multicollinearity among the predictors. Variables with high VIF values are eliminated to improve the model. 

**Step 5: Building the Adjusted Model** 
After removing variables with high multicollinearity, we build an adjusted model. This refined model is subjected to individual T-tests to evaluate the significance of the remaining predictors. This step ensures that only the most relevant variables are included in the model. 

**Step 6: Model Selection Procedures** 
Before proceeding to the interaction model, we run VIF again with the adjusted model to verify multicollinearity. We also apply model selection procedures, such as the All-Possible-Regressions Selection Procedures -Adjusted $R^2$ or RSE Criterion, Mallows Cp Criterion and Akaike information criterion (AIC) to identify the most significant predictors from the first-order model. 

**Step 7: Building the Interaction Model** 
Next, we build an interaction model to explore the interactions between predictors. We perform hypothesis tests to identify significant interaction terms and adjust the model accordingly. This step helps in capturing the combined effect of multiple predictors on the dependent variable. 

**Step 8: Testing Linearity of the Interaction Model** 
To ensure the linearity of the final interaction model, we interpret the residual plot. This involves checking for patterns in the residuals that might indicate non-linearity. If necessary, we adjust improve the model's linearity. 

**Step 9: Improving Linearity with Transformations** 
To further improve the linearity of the model, we add polynomial terms or apply log transformations to potential predictors. These transformations help in capturing non-linear relationships and enhance the model's efficiency. 

**Step 10: Testing for Homoscedasticity** 
We run the Breusch-Pagan test to check for homoscedasticity, which ensures that the variance of the residuals is constant across all levels of the independent variables. This step is crucial for validating the assumptions of linear regression. 

**Step 11: Verifying Normality** 
Using the Shapiro-Wilks test, we verify the normality of the residuals. This test helps in confirming that the residuals are normally distributed, which is an important assumption for linear regression models. 

**Step 12: Finalizing the Model, Making Interpretations and  Predictions** 
After verifying all assumptions and making necessary adjustments, we finalize the model and make interpretations of the model. The final step involves using the model to make predictions on new data. This step demonstrates the practical application of the model and its ability to provide actionable insights. 

### 2.4 Contributions

<u>**Jackson:**</u> Developed Project Introduction and Defined objectives, Interpretation and Conclusion 

<u>**Venkateshwaran:**</u> Expanded the Project Methodology, Categorized the dataset variables and defined the workflow/Task list for the models being developed and Tested 

<u>**Steen:**</u> Model with Quantitative variables - Responsible for building the full model with interaction terms that will focus on exploring only the quantitative variables (Age, Current Weight, BMR, Daily Calories Consumed, Daily Caloric Surplus/Deficit, Weight Change, Duration, and Final Weight). 

<u>**Harpreet:**</u> Model with Qualitative Variables ‚Äì Concentrate on assessing the roles played by these qualitative variables (Gender, Physical Activity Level, Sleep Quality, and Stress Level) in the model, detailing how they modify or account for differences in the dependent variable. 

<u>**Aaron:**</u> Model with all variables - Responsible for building the full model with interaction terms that will focus on exploring relationships among both qualitative and quantitative variables. A multiple regression model with interaction terms will allow them to assess not only the individual impact of each predictor on the outcome variable but also to examine how the effect of one predictor might change depending on the levels of another (e.g., stress level affecting the relationship between physical activity level and weight change). 

## 3 Main Results of the Analysis

To demonstrate the importance of including all variable types in our predictive model, we decided to create and compare three models using multiple linear regression modelling:

1. A quantitative model
2. A qualitative model
3. A model containing quantitative and qualitative variables

It should also be stated that any statistical hypothesis tests will use an $\alpha$ value of 0.05, unless otherwise indicated.

Before creating any of our models, we made sure the appropriate libraries were uploaded, and the dataset was uploaded.
```{r, echo=FALSE}
library(ggplot2)
library(GGally)
library(car)
library(lmtest)
library(MASS)
library(mctest)
library(leaps)
library(olsrr)
library(Ecdat)
weight=read.csv("https://raw.githubusercontent.com/aarongelf/Data-603-Group-Project/refs/heads/main/weight_change_dataset.csv")
print(head(weight))
```

If there is an issue uploading the dataset from the github link that has been used, a copy of the dataset has been included in our submission, that can be manually uploaded.

### 3.1 Quantitative Model

To construct our quantitative model, we begin by inspecting our dataset to determine which variables should be included.

```{r, echo=FALSE}
colnames(weight)
```

Variables under consideration:

 [1] "Participant.ID"               
 [2] "Age"                          
 [3] "Gender"                       
 [4] "Current.Weight..lbs."         
 [5] "BMR..Calories."               
 [6] "Daily.Calories.Consumed"      
 [7] "Daily.Caloric.Surplus.Deficit"
 [8] "Weight.Change..lbs."          
 [9] "Duration..weeks."             
[10] "Physical.Activity.Level"      
[11] "Sleep.Quality"                
[12] "Stress.Level"                 
[13] "Final.Weight..lbs." 

We will begin by excluding Participant.ID and any qualitative variables, leaving us with:

 [1] "Age"                          
 [2] "Current.Weight..lbs."         
 [3] "BMR..Calories."               
 [4] "Daily.Calories.Consumed"      
 [5] "Daily.Caloric.Surplus.Deficit"
 [6] "Weight.Change..lbs."          
 [7] "Duration..weeks."             
[8] "Final.Weight..lbs." 

We will then test that at least one of our predictors is significant using the following hypothesis:

$H_0: \beta_1 = \beta_2 =...= \beta_i=0\mbox{    (i=1,2,...,p)} \\$
$H_a: \mbox{ At least one } \beta_i \neq 0\mbox{    (i=1,2,...,p)}$

```{r, echo=FALSE}
quant_weight_model_full = lm(Weight.Change..lbs. ~ Age + `Current.Weight..lbs.` + `BMR..Calories.` + `Daily.Calories.Consumed` + `Daily.Caloric.Surplus.Deficit` + Duration..weeks. + `Final.Weight..lbs.`, data = weight)

summary(quant_weight_model_full)
```

The predictors Current.Weight..lbs. and Final.Weight..lbs. are highly significant but when used in relation to Weight.Change..lbs. are too correlated. Hence, we will remove them because weight change is calculated using the current weight and the final weight.

```{r, echo=FALSE}
quant_weight_model_take2 = lm(Weight.Change..lbs. ~ Age + `BMR..Calories.` + 
                         `Daily.Calories.Consumed` + `Daily.Caloric.Surplus.Deficit` + Duration..weeks., data = weight)
summary(quant_weight_model_take2)
```

After removing Current.Weight..lbs. and Final.Weight..lbs, we see no statistically significant predictors and we have an adjusted \(R^2\) of -0.014 which suggests that the remaining predictors have limited explanatory value. This low adjusted \(R^2\) indicates that more refinement is needed.

As we are still uncertain whether the model has multicollinearity, we will now test the remaining variables for that.

```{r, echo=FALSE}
vif(quant_weight_model_take2)
imcdiag(quant_weight_model_take2, method="VIF")
```

After checking the VIF, we have the result: Age = \(1.069\), BMR..Calories. = \(8.186 \times 10^7\), Daily.Calories.Consumed = \(1.626 \times 10^8\), Daily.Caloric.Surplus.Deficit = \(8.519 \times 10^7\), and Duration..weeks. = \(1.035\). Values greater than 10 suggest severe multicollinearity, hence BMR..Calories., Daily.Calories.Consumed, and Daily.Caloric.Surplus.Deficit are caught in VIF detection and need to be addressed. Based on our data dictionary we can see that Daily.Caloric.Surplus.Deficit is the difference between Daily.Calories.Consumed and BMR..Calories., therefore we have chosen to keep Daily.Caloric.Surplus.Deficit while removing the other two variables. Leaving us with the model below:

```{r, echo=FALSE}
quant_weight_model_take3 = lm(Weight.Change..lbs. ~ Age + `Daily.Caloric.Surplus.Deficit` + Duration..weeks., data = weight)
summary(quant_weight_model_take3)
```

From our output we can see that the p-value of our model is quite large, at 0.542, and none of our predictors our significant. We can attempt a stepwise selection to see if any predictors should be included.

**NOTE** The code line below is only included to demonstrate the code that was used. The output returns an error indicating that none of the variables are appropriate to select, and allowing that error to run causes our document not to knit, thus the reason it was masked.
```{r}
#stepmod=ols_step_both_p(quant_weight_model_take3, p_enter=0.05,p_remove=0.3,details=TRUE)
```


Based on our output, the large p-value of our model, and the fact that no predictors have a p-value < 0.05, we conclude that this model is not significantly different than a model with no predictors. This suggests that we should attempt other modelling approaches, such as a generalized additive model.  However, as this is not covered in the scope of our course, we will simply conclude that using only the quantitative variables is not a good method for determining weight change given our dataset.

### 3.2 Qualitative Model

```{r, echo=FALSE}
print(head(weight))
```

Meet our, 100 participants who has decided to embark on a transformative journey to improve their health and well-being. They are determined to contribute to body test which is going to be really helpful in the health department to understand how various lifestyle factors influence the body weight.

For the purposes of this section, we will be creating a model that focuses on qualitative variables, starting with the following variables -

1.  Gender- M or F
2.  Physical.Activity.Level
3.  Sleep.Quality
4.  Stress.Level

Both genders commit to track their progress with how their physical activity level is throughout that period of time. Additionally, mental aspects were recorded such as their sleep quality and stress levels.

We can determine whether our model contains any significant predictors using the hypothesis:

$H_0: \beta_1 = \beta_2 =...= \beta_i=0\mbox{    (i=1,2,...,p)}\\$ 
$H_a: \mbox{ At least one } \beta_i \neq 0\mbox{    (i=1,2,...,p)}$

```{r, echo=FALSE}
Weight_full_model= lm(Weight.Change..lbs.~ factor(Gender)+factor(Physical.Activity.Level)+factor(Sleep.Quality)+factor(Stress.Level), data=weight)
print(summary(Weight_full_model))
```
Based on our output, we can see that there is at least on predictor with a p-value < 0.05, allowing us to reject the null hypothesis and conclude that at least one predictor is significant.

To determine which predictors are significant, we can test the hypothesis:

$H_0: \beta_i =0 \mbox{    (i=1,2,...,p)} \\$
$H_a: \beta_i \neq 0\mbox{    (i=1,2,...,p)}$

From our output above we can see that the predictors Sleep.Quality and Stress.Level contain at least one factor with a p-value < 0.05. We therefore reject the null hypothesis and conclude that Sleep.Quality and Stress.Level significantly affect weight change and should be included in our model.

```{r, echo=FALSE}
weight_refined_model=lm(Weight.Change..lbs.~factor(Sleep.Quality)+factor(Stress.Level), data=weight)
print(summary(weight_refined_model))
```
From our output we can see that our refined model has a slightly better **Adjusted R-squared** (0.69) than the full model (0.688), indicating an improvement in model simplicity and explanatory power.

The variables **factor(Sleep.Quality)Poor**, **factor(Stress.Level)8**, and **factor(Stress.Level)9** remain highly significant and have the greatest impact on weight change.

Ton ensure no issues with multicollinearity we can run a VIF test on our model

```{r, echo=FALSE}
vif(weight_refined_model)
imcdiag(weight_refined_model,method='VIF')
```
From our output we can see that the VIF is less than 5 so all predictors are kept as there is no indication of  significant multicollinearity.

Now that we have concluded the absence of multicollinearity, we can begin strengthening our model. To begin, we will look at adding interaction terms. Interaction terms are useful when we suspect that the effect of one predictor on the response variable depends on the level of another predictor.

```{r, echo=FALSE}
weight_interaction_model= lm(Weight.Change..lbs.~(factor(Sleep.Quality)+factor(Stress.Level))^2, data=weight)
print(summary(weight_interaction_model))
```

The interaction model does not improve predictive performance or interpretability over the refined model. The refined model remains the best choice for explaining the relationship between weight change, Sleep Quality, and Stress Level.

Furthermore, the Adjusted R sqaure in our interaction model has decreased from our refined model (0.656, down from 0.69), and the RSE has increased (4.367, up from 4.145). We also see that only one predictor has a p-value < 0.05. Based on all of this, we conclude that the refined model is better than the presented interaction model.


To further improve our model we can attempt the addition of higher order terms.

**Note** Use of I(factor()\^2) is for continuous variables where you want to square the variable, but it's not applicable to categorical variables. As here we have all categorical variables we will just do it like -

```{r, echo=FALSE}
weight_higher_order_model = lm(Weight.Change..lbs. ~ (factor(Sleep.Quality) + factor(Stress.Level))^3, data=weight)
print(summary(weight_higher_order_model))
```

Notice that this gives the same output as before. Similar to before we will stick with our refined model.

Now we will test for some of the assumptions in our model, using plots and stastical tests.

We can test for linearity by inspecting our residual plots.

```{r, echo=FALSE}
# Residuals vs Fitted Values
ggplot(weight_refined_model, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_smooth(method = "loess", color = "blue") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Figure 1. Residuals vs Fitted Values",
       x = "Fitted Values", y = "Residuals")
```
Figure 1: Residuals vs Fitted Values plot for the refined weight model to test for linearity.

This plot suggests that the model does not quite pass the assumption of linearity. We can see that the residuals do not appear randomly scattered, rather they are clustered in 3 different groups, additionally we can see the presence of a curve in our line. Normally we could try transforming our data, but in this case, as mentioned before, we cannot add higher order terms. We also cannot perform a log transformation on our predictors or our responding variable, since our dummy variables contain 0 and our responding variable contains negative values.

To test our independence assumption, we can inspect the residuals in the figure below.

```{r, echo=FALSE}
plot(residuals(weight_refined_model), type = "o", col = "blue", 
     main = "Figure 2. Residuals vs Observation Number",
     xlab = "Observation Number", ylab = "Residuals")
abline(h = 0, lty = 2, col = "red")
```
Figure 2. Residuals vs observation number plot for our refined weight model to test for independence.

Given the randomly scattered point, and lack of trends, we can suggest that independence has most likely been met.  Although there a few outliers, they do not appear to follow any pattern. Additionally, our responding variable is not considered time-series data.

To test for equal variance, we can inspect our residual plots and perform a Breusch-Pagan Test, where the hypothesis would be:
$$
H_0: \sigma^2_1 = \sigma^2_2 = ...=\sigma^2_n\mbox { (heteroscedasticity is not present)} \\
H_a: \mbox {at least one }\sigma^2_i \mbox{ is different from the others i = 1, 2, ..., n (heteroscedasticity is present)}
$$

```{r, echo=FALSE}
ggplot(weight_refined_model, aes(x = .fitted, y = .resid)) +
  geom_point(color = "purple") +
  geom_smooth(method = "loess", color = "green4") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Figure 3. Residuals vs Fitted Values", x = "Fitted Values", y = "Residuals")

# Breusch-Pagan Test
library(lmtest)
bptest(weight_interaction_model)
```
Figure 3. Residuals vs fitted plot for our refined weight model to test for heteroscedasticity.

As we can see the presence of patterns in our data points, and the line is curved, we believe heteroscedasticity is present. Our Breusch-Pagan test returned a p-value of 0.019, which is less than 0.05. Therefore we reject the null hypothesis, and conclude there is statistically significant evidence of heteroscedasticity in the model.

To test our assumptions of normality, we can create our Q-Q plot and run a Shapiro-Wilks test, using the the hypothesis:
$$
\begin{aligned}
H_0 &: \mbox{the residuals are normally distributed} \\
H_a &: \mbox{the residuals are not normally distributed}
\end{aligned}
$$

```{r, echo=FALSE}
# QQ Plot
qqnorm(residuals(weight_refined_model), main = "Figure 4. Normal Q-Q Plot")
qqline(residuals(weight_refined_model), col = "red")

# Shapiro-Wilk Test
shapiro.test(residuals(weight_refined_model))
```
Figure 4. Normal Q-Q plot for the residuals of our refined weight model to test that our residuals are normally distributed.

The Q-Q plot suggests that the data may not be perfectly normally distributed. The outliers might be indicative of non-normality. As our Shapiro-Wilks test returned a p-value of 3.793e-09, which is < 0.05, we reject the null hypothesis and conclude that our residuals are not normally distributed.

To check for outliers we can look at our Cook's Distance and Leverage plots.

```{r, echo=FALSE}
# Cook's Distance
plot(weight_refined_model, which = 4, col = "red", pch = 18, 
     main = "Figure 5. Cook's Distance Plot")

# Observations with high Cook's distance
weight[cooks.distance(weight_refined_model) > 0.5, ]

# Leverage values
lev = hatvalues(weight_refined_model)
p = length(coef(weight_refined_model))
n = nrow(weight)
outliers_2pn = lev[lev > (2 * p / n)]
outliers_3pn = lev[lev > (3 * p / n)]
```
Figure 5. Cook's distance plot for our refined weight model to test for the presence of outliers.

```{r, echo=FALSE}
# Leverage plot
plot(1:n, lev, type = "h", main = "Figure 6. Leverage Plot", 
     xlab = "Observation Index", ylab = "Leverage")
abline(h = c(2 * p / n, 3 * p / n), col = c("blue", "red"), lty = 2)
legend("topright", legend = c("2p/n", "3p/n"), col = c("blue", "red"), lty = 2)
```
Figure 6. Leverage plot for our refined weight model to determine which points strongly influence the slope of the least squares line.

There are three influential points in the data, highlighted by the red vertical lines at observations 8, 38, and 72. Cook's Distance Values: These points have high Cook's distances, exceeding the threshold, suggesting they significantly influence the regression model.

High leverage: The outlier has an unusual combination of predictor values, making it influential. High residual: The outlier has a large difference between its observed and predicted values, suggesting it's not well explained by the model.

The data points at observations 8, 38, and 72 are influential because they significantly affect the estimated coefficients of the regression model. 

Although all of our assumptions were not met, we will still present our final model.

```{r, echo=FALSE}
print(summary(weight_refined_model))
```

$$
\begin{aligned}
\text{Weight.Change (lbs)} &= 1.668 + 0.667 \cdot \text{SleepQualityFair} + 0.867 \cdot \text{SleepQualityGood} \\
&\quad - 7.900 \cdot \text{SleepQualityPoor} - 0.167 \cdot \text{StressLevel2} + 0.721 \cdot \text{StressLevel3} \\
&\quad + 1.124 \cdot \text{StressLevel4} + 1.458 \cdot \text{StressLevel5} - 0.054 \cdot \text{StressLevel6} \\
&\quad - 0.141 \cdot \text{StressLevel7} - 11.342 \cdot \text{StressLevel8} - 9.379 \cdot \text{StressLevel9}
\end{aligned}
$$


In conclusion the refined model explores the relationship between weight change and two key factors: sleep quality and stress level. 

Now that we have our final model, we can begin interpreting it.

Intercept: When sleep quality is excellent, and our stress level is 1, we can expect weight change to increase by 1.668 pounds.

Sleep Quality: 

- Participants with fair sleep quality gain 0.667 pounds more weight on average compared to those with excellent sleep quality.
- Participants with good sleep quality gain 0.867 pounds more weight on average compared to those with excellent sleep quality.
- Participants with poor sleep quality lose 7.900 pounds more weight on average compared to those with excellent sleep quality.

Stress Level:

- Participants with a stress level of 2 lose 0.167 pounds more weight on average compared to those with a stress level of 1.
- Participants with a stress level of 3 gain 0.721 pounds more weight on average compared to those with a stress level of 1.
- Participants with a stress level of 4 gain 1.124 pounds more weight on average compared to those with a stress level of 1.
- Participants with a stress level of 5 gain 1.458 pounds more weight on average compared to those with a stress level of 1.
- Participants with a stress level of 6 lose 0.054 pounds more weight on average compared to those with a stress level of 1.
- Participants with a stress level of 7 lose 0.141 pounds more weight on average compared to those with a stress level of 1.
- Participants with a stress level of 8 lose 11.342 pounds more weight on average compared to those with a stress level of 1.
- Participants with a stress level of 9 lose 9.379 pounds more weight on average compared to those with a stress level of 1.

These findings suggest that worse sleep quality and higher stress levels are associated with greater weight loss.

Overall, the model explains a moderate proportion of variance in weight change (Adjusted R-squared = 0.69), indicating that these two factors contribute to understanding weight changes, but other un-explored factors might also play a significant role.

### 3.3 Model Including All Variables

To determine the best model for predicting weight change, given our dataset, we begin by inspecting our dataset.
```{r, echo=FALSE}
weight=read.csv("https://raw.githubusercontent.com/aarongelf/Data-603-Group-Project/refs/heads/main/weight_change_dataset.csv")
head(weight)
```

Our first step is to build our full model, and determine whether any assumptions have been broken.  Do note, that we chose to remove the variables "Current.Weight..lbs." and "Final.Weight..lbs." because our responding variable, "Weight.Change..lbs." is just a result of subtracting the two variables, making weight change dependent on both current and final weight. To  ensure that at least one of our predictors is significant, we test the hypothesis:

$H_0: \beta_1 = \beta_2 =...= \beta_i=0\mbox{    (i=1,2,...,p)} \\$
$H_a: \mbox{ At least one } \beta_i \neq 0\mbox{    (i=1,2,...,p)}$

```{r, echo=FALSE}
weight_model_full=lm(Weight.Change..lbs. ~ Age + factor(Gender) + BMR..Calories. + Daily.Calories.Consumed + Daily.Caloric.Surplus.Deficit + Duration..weeks. + factor(Physical.Activity.Level) + factor(Sleep.Quality) + factor(Stress.Level), data=weight)
print(summary(weight_model_full))
```
Based on our output, we can see that at least one predictor has a p-value < 0.05, therefore we reject our null hypothesis and conclude that at least one predictor significantly affects weight change. Before determining which predictors to include in our model, we want to test for multicollinearity. This can be done using the VIF method.

```{r, echo=FALSE}
vif(weight_model_full)
imcdiag(weight_model_full, method="VIF")
```
From our output, we can see that three variables have a VIF score > 5, suggesting multicollinearity. These variables are "BMR..Calories.", "Daily.Calories.Consumed", and "Daily.Caloric.Surplus.Deficit". Based on the definitions provided in the data set, "Daily.Caloric.Surplus.Deficit" is the difference between calories consumed and BMR, therefore we chose to keep "Daily.Caloric.Surplus.Deficit" and remove "BMR..Calories.", and "Daily.Calories.Consumed".

Following the removal of those variables, we will determine which predictors are significant using the hypothesis test:

$H_0: \beta_i =0 \mbox{    (i=1,2,...,p)} \\$
$H_a: \beta_i \neq 0\mbox{    (i=1,2,...,p)}$

This can be achieved by looking at the individual t-tests from the summary of our adjusted model.
```{r, echo=FALSE}
weight_model_adj=lm(Weight.Change..lbs.~Daily.Caloric.Surplus.Deficit+Age+factor(Gender)+Duration..weeks.+factor(Physical.Activity.Level)+factor(Sleep.Quality)+factor(Stress.Level), data=weight)
summary(weight_model_adj)
```
Based on this output, we can see there are 4 instances where the null hypothesis is rejected (p-value < 0.05), implying that these variables are significant and should be included in our model.  These variables are:

- Duration..weeks.
- factor(Sleep.Quality)Poor
- factor(Stress.Level)8
- factor(Stress.Level)9

Despite the fact that only some of the dummies are significant, for the categories "Sleep.Quality" and "Stress.Level", we must include the full variable in our model.  Therefore our model will include the predictors Sleep.Quality, Stress.Level and Duration..weeks.

**NOTE** An attempt was made to use stepwise regression to determine which variables were best to keep, with the code for that being included in the appendix. Upon running the stepwise, it was determined that a 4th variable, Daily,Caloric.Surplus.Deficit should be included.  In the end we chose not to include this variable for a couple reasons. First, our categorical variables had many levels, in which case it is often better to just manually select significant predictors using t-tests from our summary table. When performing this manual selection, we found the variable the stepwise selection wanted to include, had a p-value 0f 0.156, and a coefficient of 0.003.  This implied the variable had no significant effect. Based on this, and to avoid overfitting and complicating our model, we chose to not include this variable

```{r, echo=FALSE}
weight_model_adj2=lm(Weight.Change..lbs.~factor(Sleep.Quality)+Duration..weeks.+factor(Stress.Level), data=weight)
summary(weight_model_adj2)
```
We can see that all predictors are now significant.  Before moving on to interaction terms, we should check again for multicollinearity, just to be certain:

```{r, echo=FALSE}
vif(weight_model_adj2)
imcdiag(weight_model_adj2, method="VIF")
```
As we can see from our output, there are no issues with multicollinearity.

Now we can begin checking for interaction terms. We can start by testing all interaction terms for our model using the hypothesis test:

$H_0: \beta_{interaction} =0 \mbox{    (i=1,2,...,p)} \\$
$H_a: \beta_{interaction} \neq 0\mbox{    (i=1,2,...,p)}$

```{r, echo=FALSE}
weight_model_int=lm(Weight.Change..lbs.~(factor(Sleep.Quality)+Duration..weeks.+factor(Stress.Level))^2, data=weight)
summary(weight_model_int)
```
Based on our output, we can see the significant variables (with a p-value <0.05) are:

- 'factor(Sleep.Quality)Poor:Duration..weeks.'
- 'Duration..weeks.:factor(Stress.Level)8'
- 'Duration..weeks.:factor(Stress.Level)9'.  

Although none of the initial predictors are considered significant, they are all parts of significant interaction terms, and must therefore be included in the model, therefore our interaction model would include 'Sleep.Quality', 'Duration..weeks.', 'Stress.Level', 'Sleep.Quality':'Duration..weeks.', and 'Duration..weeks.':'Stress.Level'.

```{r, echo=FALSE}
weight_model_int_final=lm(Weight.Change..lbs.~factor(Sleep.Quality)+Duration..weeks.+factor(Stress.Level)+factor(Sleep.Quality)*Duration..weeks.+Duration..weeks.*factor(Stress.Level), data=weight)
summary(weight_model_int_final)
```
Our final model, before testing for the rest of our assumptions, will include the following predictors and interaction terms: Sleep.Quality, Duration..weeks., Stress.Level, Sleep.Quality:Duration..weeks., and Duration..weeks.:Stress.Level.

Based on this model, we can now test the rest of our assumptions. We will start with testing for linearity by inspecting our residual plots.
```{r, echo=FALSE}
ggplot(weight_model_int_final, aes(x=.fitted, y=.resid)) +
geom_point() +geom_smooth()+
geom_hline(yintercept = 0)+
  labs(title = "Figure 7. Residuals vs Fitted Values",
       x = "Fitted Values", y = "Residuals")
```
Figure 7: Residuals vs fitted values plot for the interaction model to test for linearity.

Based on our residuals, there appears to be a pattern, with most data points on the right side, as well as the presence of some funnel shape. We can try to correct this by adding some polynomial terms, or transforming the data. We will start with polynomial terms.
```{r, echo=FALSE}
weight_model_poly=lm(Weight.Change..lbs.~factor(Sleep.Quality)+Duration..weeks.+I(Duration..weeks.^2)+factor(Stress.Level)+factor(Sleep.Quality)*Duration..weeks.+Duration..weeks.*factor(Stress.Level), data=weight)
summary(weight_model_poly)
```

```{r, echo=FALSE}
ggplot(weight_model_poly, aes(x=.fitted, y=.resid)) +
geom_point() +geom_smooth()+
geom_hline(yintercept = 0)+
  labs(title = "Figure 8. Residuals vs Fitted Values",
       x = "Fitted Values", y = "Residuals")
```
Figure 8: Residuals vs fitted values plot for our polynomial model to test for any improvements to linearity.

Adding a polynomial term appears to not be the solution, as not only does the residual plot appear the same, but the adjust R-sqaured barely increases. There is no point to preventing our interpretation for such a small increase.

Next we will try performing a log transformation on Duration..weeks.

```{r, echo=FALSE}
weight_model_log <- lm(Weight.Change..lbs. ~ factor(Sleep.Quality) + log(Duration..weeks.) + factor(Stress.Level) + factor(Sleep.Quality) * log(Duration..weeks.) + log(Duration..weeks.) * factor(Stress.Level), data = weight)
summary(weight_model_log )
```

```{r, echo=FALSE}
ggplot(weight_model_log, aes(x=.fitted, y=.resid)) +
geom_point() +geom_smooth()+
geom_hline(yintercept = 0)+
  labs(title = "Figure 9. Residuals vs Fitted Values",
       x = "Fitted Values", y = "Residuals")
```
Figure 9: Residuals vs fitted values plot for our log transformed model to test for any improvements to linearity.

Transforming Duration..weeks. using a log transformation appears to not correct the linearity problem either.  In fact the residual plot now looks worse than before.

Based on these attempts, we may need to add more complex polynomial or transformations to our data. We may also need to attempt some other model types, but for the sake of this class we will move forward using our model before we attempting correcting linearity, and simply state that our model fails the linearity assumption.

The next assumption we will look at is independence. As our responding variable, Weight.Change..lbs. is not considered time-series data we can conclude this assumption has been passed.

To test for homoscedasticity we can look at our residual plots, scale-location plots, and run a Breusch-Pagan Test, using the following hypothesis:
$$
\begin{aligned}
H_0 &: \sigma^2_1 = \sigma^2_2 = ...=\sigma^2_n\mbox { (heteroscedasticity is not present)} \\
H_a &: \mbox {at least one }\sigma^2_i \mbox{ is different from the others i = 1, 2, ..., n (heteroscedasticity is present)}
\end{aligned}
$$

```{r, echo=FALSE}
ggplot(weight_model_int_final, aes(x=.fitted, y=.resid)) +
geom_point() +geom_smooth()+
geom_hline(yintercept = 0)+
  labs(title = "Figure 10. Residuals vs Fitted Values",
       x = "Fitted Values", y = "Residuals")
```
Figure 10. Residuals vs fitted values plot for our interaction model to test for homoscedasticity.

```{r, echo=FALSE}
plot(weight_model_int_final,which=3, main="Figure 11. Scale-Location Plot")
bptest(weight_model_int_final)
```
Figure 11. Scale-Location plot for our interaction model to test for homoscedasticity.

Based on our plots and the fact that our p-value for our Breusch-Pagan Test (0.004) is < 0.05, we can reject our null hypothesis, implying the presence of heteroscedasticity. To try and correct this, we can attempt a log transformation.

```{r, echo=FALSE}
ggplot(weight_model_log, aes(x=.fitted, y=.resid)) +
geom_point() +geom_smooth()+
geom_hline(yintercept = 0)+
  labs(title = "Figure 12. Residuals vs Fitted Values",
       x = "Fitted Values", y = "Residuals")
```
Figure 12. Residuals vs fitted values plot for our log transformed model to test for improvements to homoscedasticity.

```{r, echo=FALSE}
plot(weight_model_log,which=3, main="Figure 13. Scale-Location Plot")
bptest(weight_model_log)
```
Figure 13. Scale-Location plot for our log transformed model to test for improvements to homoscedasticity.

Although, our residual plot does not look to have improved much, our scale-location plot looks much better now than before, and we can see that our p-value for our Breusch-Pagan Test (0.148) is > 0.05. This means that we now fail to reject our null hypotheis, implying heteroscedasticity is not present. This suggests that our log transformation has improved our model, allowing it to now pass the homoscedasticity assumption.  It should be noted that the adjusted R-square of our log model has decreased slightly from our previous model (0.846 down from 0.856).  Despite this loss, we are comfortable moving forward with the log model, as it helps us pass one of our assumptions that was previously not met.

To test whether our residuals are normally distributed, we can inspect our Q-Q plot, as well as run a Shapiro-Wilk test, using the following hypothesis:
$$
\begin{aligned}
H_0 &: \mbox{the residuals are normally distributed} \\
H_a &: \mbox{the residuals are not normally distributed}
\end{aligned}
$$

```{r, echo=FALSE}
plot(weight_model_log,which=2, main="Figure 14. Normal Q-Q Plot")
shapiro.test(residuals(weight_model_log))
```
Figure 14. Normal Q-Q plot for the residuals of our log transformed model to test that our residuals are normally distributed.

Based on our Q-Q plot, we can see that the residuals are likely not normally distributed. This is further reinforced by our Shapiro Wilks test, which returned a p-value 1.788e-08.  As this is < 0.05, we reject our null hypothesis, implying that the residuals are not normally distributed.  We can attempt to correct this using a Box-Cox transformation.  Unfortunately, as our responding variable is weight change, there are values less than 0. Therefore a Box-Cox transformation cannot be done. For the time being we will simply conclude that our model has not passed the assumption of normality.

Finally, we will check for outliers, using our residuals vs. leverage plot.

```{r, echo=FALSE}
plot(weight_model_log,which=5, main="Figure 15. Residuals vs Leverage Plot")
```
Figure 15. Residuals vs leverage plot of our log transformed model to test for the presence of outliers.

Based on our plot, we can see that observation 49 is an outlier. We can attempt to remove the data point to see how it affects our model.

```{r, echo=FALSE}
weight_clean=weight[!weight$Participant.ID==49,]
weight_model_log_cleaned=lm(Weight.Change..lbs. ~ factor(Sleep.Quality) + log(Duration..weeks.) + factor(Stress.Level) + factor(Sleep.Quality) * log(Duration..weeks.) + log(Duration..weeks.) * factor(Stress.Level), data = weight_clean)
summary(weight_model_log_cleaned)
```
Based on the output of our model with and without the outlier, we can see that removing the outlier slightly improved our adjusted R-squared (0.851 from 0.846), while reducing or RSE (2.889 from 2.917), without changing the significance of any of our predictors.  Based on this we feel comfortable removing the indicated outlier.

Although our model does fail the assumptions of linearity and normality, for the sake of this project we will go forward with our final model being:

$$
\begin{aligned}
X_{\text{Weight.Change..lbs.}} &= 0.21766 + 0.18485 X_{\text{Sleep.Quality}_{\text{Fair}}} - 1.59689 X_{\text{Sleep.Quality}_{\text{Good}}} \\
&\quad + 1.24320 X_{\text{Sleep.Quality}_{\text{Poor}}} + 1.21315 X_{\log(\text{Duration..weeks.})} + 0.54944 X_{\text{Stress.Level}_{2}} \\
&\quad - 2.44803 X_{\text{Stress.Level}_{3}} - 1.93746 X_{\text{Stress.Level}_{4}} - 4.19952 X_{\text{Stress.Level}_{5}} \\
&\quad - 0.32160 X_{\text{Stress.Level}_{6}} - 2.49840 X_{\text{Stress.Level}_{7}} + 13.14533 X_{\text{Stress.Level}_{8}} \\
&\quad - 0.12103 X_{\text{Stress.Level}_{9}} - 0.06075 (X_{\text{Sleep.Quality}_{\text{Fair}}} X_{\log(\text{Duration..weeks.})}) \\
&\quad + 0.55598 (X_{\text{Sleep.Quality}_{\text{Good}}} X_{\log(\text{Duration..weeks.})}) - 5.29987 (X_{\text{Sleep.Quality}_{\text{Poor}}} X_{\log(\text{Duration..weeks.})}) \\
&\quad - 0.46614 (X_{\log(\text{Duration..weeks.})} X_{\text{Stress.Level}_{2}}) + 1.45515 (X_{\log(\text{Duration..weeks.})} X_{\text{Stress.Level}_{3}}) \\
&\quad + 0.89513 (X_{\log(\text{Duration..weeks.})} X_{\text{Stress.Level}_{4}}) + 2.98467 (X_{\log(\text{Duration..weeks.})} X_{\text{Stress.Level}_{5}}) \\
&\quad - 0.02623 (X_{\log(\text{Duration..weeks.})} X_{\text{Stress.Level}_{6}}) + 1.23485 (X_{\log(\text{Duration..weeks.})} X_{\text{Stress.Level}_{7}}) \\
&\quad - 12.05261 (X_{\log(\text{Duration..weeks.})} X_{\text{Stress.Level}_{8}}) - 6.05003 (X_{\log(\text{Duration..weeks.})} X_{\text{Stress.Level}_{9}})
\end{aligned}
$$

Now that we have our final model, we can begin interpreting it.

Intercept: When sleep quality is excellent, stress level is 1, and log(duration) is 0 (which would be a duration of 1 week), the expected weight change is 0.21766 pounds.

Sleep Quality:

- If sleep quality is fair, the predicted weight change is $(0.18485 - 0.06075 X_{log(Duration..weeks.)})$ pounds.
- If sleep quality is good, the predicted weight change is $(-1.59689 + 0.55598 X_{log(Duration..weeks.)})$ pounds.
- If sleep quality is poor, the predicted weight change is $(1.24320 -5.29987 X_{log(Duration..weeks.)})$ pounds.
- If sleep quality is excellent, the predicted weight change would simply be our model equation excluding any $Sleep.Quality_i$ terms or interactions, where i = Fair, Good, or Poor, as all values would simply be 0 for those variables.

Duration in weeks:

- For every one-unit increase in log(Duration..weeks.), weight change increases by: 
$$
\begin{aligned}
&(1.213 - 0.061 X_{\text{Sleep.Quality}_{\text{Fair}}} + 0.556 X_{\text{Sleep.Quality}_{\text{Good}}} - 5.300 X_{\text{Sleep.Quality}_{\text{Poor}}} \\
&- 0.466 X_{\text{Stress.Level}_{2}} + 1.455 X_{\text{Stress.Level}_{3}} + 0.895 X_{\text{Stress.Level}_{4}} \\
&+ 2.985 X_{\text{Stress.Level}_{5}} - 0.026 X_{\text{Stress.Level}_{6}} + 1.235 X_{\text{Stress.Level}_{7}} \\
&- 12.053 X_{\text{Stress.Level}_{8}} - 6.050 X_{\text{Stress.Level}_{9}}) \, \text{pounds}
\end{aligned}
$$


Stress Level:

- If stress level is 2, the predicted weight change is $(0.549 - 0.466 X_{log(Duration..weeks.)})$ pounds.
- If stress level is 3, the predicted weight change is $(-2.448 +1.455 X_{log(Duration..weeks.)})$ pounds.
- If stress level is 4, the predicted weight change is $(-1.937 +0.895 X_{log(Duration..weeks.)})$ pounds.
- If stress level is 5, the predicted weight change is $(-4.200 +2.985 X_{log(Duration..weeks.)})$ pounds.
- If stress level is 6, the predicted weight change is $(-0.322 -0.026 X_{log(Duration..weeks.)})$ pounds.
- If stress level is 7, the predicted weight change is $(-2.498 +1.235 X_{log(Duration..weeks.)})$ pounds.
- If stress level is 8, the predicted weight change is $(13.145 -12.053 X_{log(Duration..weeks.)})$ pounds.
- If stress level is 9, the predicted weight change is $(-0.121 -6.050 X_{log(Duration..weeks.)})$ pounds.
- If stress level is 1, the predicted weight change would simply be our model equation excluding any $Stress.Level_i$ terms or interactions, where i = 2 to 9, as all values would simply be 0 for those variables.

Lastly, our Adjusted $R^2$ for our model is 0.851, implying that our 85.1% of the variability in weight change is explained by our model.

To demonstrate the predictiveness of our model, we have included an example. Assume a participant records their weight change over a period of 6 weeks, where they recorded poor sleep quality, and a stress level of 5. What would be there predicted weight change?

```{r, echo=FALSE}
new_weight_data=data.frame(Sleep.Quality='Good',Stress.Level=5,Duration..weeks.=6)
predict(weight_model_log_cleaned,new_weight_data,type="response")
```
Based on our finalized model, and assuming a participant records their weight change over a period of 6 weeks, where they recorded poor sleep quality, and a stress level of 5, we would predict their weight to decrease by 4.713 pounds.

```{r}
new_weight_data=data.frame(Sleep.Quality='Poor',Stress.Level=9,Duration..weeks.=9)
predict(weight_model_log_cleaned,new_weight_data,type="response")
```


## 4 Conclusion and Discussion

In this project, we examined the impacts that diet, lifestyle, and demographic factors have on weight change. Using multi-linear regression, we compared quantitative, qualitative, and combined variable models to identify any significant predictors that affect fluctuations in weight change.

Our findings show the large role stress levels and sleep quality play on weight change, particularly how poor sleep and elevated stress levels have a more pronounced impact. Participants from this study tended to experience more weight loss when connected with poor sleep. While weight loss tended to display a wider range for stress levels, we observed much consistent higher weight loss for the highest stress levels. These results demonstrate how both physical and mental health causes difficulties when it comes to weight management.

Although our combined model provides the most comprehensive result, it still poses challenges such as with multicollinearity as well as unmet regression assumptions causing limited explanation when it comes to our quantitative predictors. Regardless, after refining the model we should have statistical significance.

Despite the fact that some assumptions were not met, and could not be corrected with transformations, we were able to arrive at a final model, with an equation of:

$$
\begin{aligned}
X_{\text{Weight.Change..lbs.}} &= 0.21766 + 0.18485 X_{\text{Sleep.Quality}_{\text{Fair}}} - 1.59689 X_{\text{Sleep.Quality}_{\text{Good}}} \\
&\quad + 1.24320 X_{\text{Sleep.Quality}_{\text{Poor}}} + 1.21315 X_{\log(\text{Duration..weeks.})} + 0.54944 X_{\text{Stress.Level}_{2}} \\
&\quad - 2.44803 X_{\text{Stress.Level}_{3}} - 1.93746 X_{\text{Stress.Level}_{4}} - 4.19952 X_{\text{Stress.Level}_{5}} \\
&\quad - 0.32160 X_{\text{Stress.Level}_{6}} - 2.49840 X_{\text{Stress.Level}_{7}} + 13.14533 X_{\text{Stress.Level}_{8}} \\
&\quad - 0.12103 X_{\text{Stress.Level}_{9}} - 0.06075 (X_{\text{Sleep.Quality}_{\text{Fair}}} X_{\log(\text{Duration..weeks.})}) \\
&\quad + 0.55598 (X_{\text{Sleep.Quality}_{\text{Good}}} X_{\log(\text{Duration..weeks.})}) - 5.29987 (X_{\text{Sleep.Quality}_{\text{Poor}}} X_{\log(\text{Duration..weeks.})}) \\
&\quad - 0.46614 (X_{\log(\text{Duration..weeks.})} X_{\text{Stress.Level}_{2}}) + 1.45515 (X_{\log(\text{Duration..weeks.})} X_{\text{Stress.Level}_{3}}) \\
&\quad + 0.89513 (X_{\log(\text{Duration..weeks.})} X_{\text{Stress.Level}_{4}}) + 2.98467 (X_{\log(\text{Duration..weeks.})} X_{\text{Stress.Level}_{5}}) \\
&\quad - 0.02623 (X_{\log(\text{Duration..weeks.})} X_{\text{Stress.Level}_{6}}) + 1.23485 (X_{\log(\text{Duration..weeks.})} X_{\text{Stress.Level}_{7}}) \\
&\quad - 12.05261 (X_{\log(\text{Duration..weeks.})} X_{\text{Stress.Level}_{8}}) - 6.05003 (X_{\log(\text{Duration..weeks.})} X_{\text{Stress.Level}_{9}})
\end{aligned}
$$

This project helped demonstrate the difficulties that may arise when limiting your modelling methods to one variable as opposed to incorporating them all, as seen with the need to halt our modelling for the quantitative model.  It also helped uncover the critical factors that not only influence weight change, but also reveals how complex human health and behavior are. To build off this analysis in the future, more in-depth analyses could be held by integrating additional datasets; for example, adding both longitudinal and temporal studies. Additionally, further investigation into how sleep and stress mitigation could be applied to public health strategies.

Overall, this project contributes to better understanding the dynamics of weight change which helps pave the way for health management plans.

## 5 References

1. Twells, L. K., Gregory, D. M., Reddigan, J., & Midodzi, W. K. (2014). Current and predicted prevalence of obesity in Canada: A trend analysis. CMAJ Open, 2(1), E18‚ÄìE26. https://doi.org/10.9778/cmajo.20130016

2. National Institute of Diabetes and Digestive and Kidney Diseases. (n.d.). Factors affecting weight & health. U.S. Department of Health and Human Services. Retrieved November 28, 2024, from https://www.niddk.nih.gov/health-information/weight-management/adult-overweight-obesity/factors-affecting-weight-health

3. Rahman, Md. H. (2024, October 22). Diabetes dataset. Kaggle. https://www.kaggle.com/datasets/hasibur013/diabetes-dataset

[^1]:Twells, L. K., Gregory, D. M., Reddigan, J., & Midodzi, W. K. (2014). Current and predicted prevalence of obesity in Canada: A trend analysis. CMAJ Open, 2(1), E18‚ÄìE26. https://doi.org/10.9778/cmajo.20130016

[^2]:National Institute of Diabetes and Digestive and Kidney Diseases. (n.d.). Factors affecting weight & health. U.S. Department of Health and Human Services. Retrieved November 28, 2024, from https://www.niddk.nih.gov/health-information/weight-management/adult-overweight-obesity/factors-affecting-weight-health

[^3]:Rahman, Md. H. (2024, October 22). Diabetes dataset. Kaggle. https://www.kaggle.com/datasets/hasibur013/diabetes-dataset

\newpage

## 6 Appendix

### 6.2 Code for Stepwise Selection for Model Including All Variables

To determine the best model for predicting weight change, given our dataset, we begin by uploading the data set.
```{r, echo=FALSE}
weight=read.csv("https://raw.githubusercontent.com/aarongelf/Data-603-Group-Project/refs/heads/main/weight_change_dataset.csv")
colnames(weight)
```

Due to limitations with the ols_step_both_p command, we need to convert our numeric categorical variable into one that will be recognized as a qualitative variable.
```{r, echo=FALSE}
weight$Stress.Level[which(weight$Stress.Level==1)]="D1"
weight$Stress.Level[which(weight$Stress.Level==2)]="D2"
weight$Stress.Level[which(weight$Stress.Level==3)]="D3"
weight$Stress.Level[which(weight$Stress.Level==4)]="D4"
weight$Stress.Level[which(weight$Stress.Level==5)]="D5"
weight$Stress.Level[which(weight$Stress.Level==6)]="D6"
weight$Stress.Level[which(weight$Stress.Level==7)]="D7"
weight$Stress.Level[which(weight$Stress.Level==8)]="D8"
weight$Stress.Level[which(weight$Stress.Level==9)]="D9"
head(weight)
```

Following this we create our full model. As we ran our VIF tests earlier, we have already removed the variables that had multicollinearity issues.
```{r, echo=FALSE}
weight_model_full=lm(Weight.Change..lbs. ~ Age + Gender + Daily.Caloric.Surplus.Deficit + Duration..weeks. + Physical.Activity.Level + Sleep.Quality + Stress.Level, data=weight)
summary(weight_model_full)
```

We then attempt a stepwise selection method to determine the best predictors
```{r, echo=FALSE}
stepmod=ols_step_both_p(weight_model_full, p_enter=0.05,p_remove=0.1,details=TRUE)
```

```{r, echo=FALSE}
ExecSubsets=ols_step_best_subset(weight_model_full, details=FALSE) # for the output interpretation 
rsquare=c((ExecSubsets$metrics)$rsquare) 
rse=c((ExecSubsets$metrics)$rse)
AdjustedR=c((ExecSubsets$metrics)$adjr)
cp=c((ExecSubsets$metrics)$cp) 
AIC=c((ExecSubsets$metrics)$aic) 
cbind(rsquare,AdjustedR,rse,cp,AIC)
```

From the stepwise selection method, the adjusted $R^2$, the cp, and the AIC, we can see that the model with 4 variables should be selected, which includes Daily.Caloric.Surplus.Deficit, Duration..weeks., Sleep.Quality, and Stress.Level. However, for reasons discussed in our final report, we choose to select our predictors using the t-test results from our full model summary.


